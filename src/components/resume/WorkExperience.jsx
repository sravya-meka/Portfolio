const WorkExperience = [
    {
        id: 1,
        title: "Data Engineer",
        company: "ExcelSoft",
        yearsActive: "Nov 2024 – Present",
        information: [
            "Engineered cloud-based ETL pipelines using Python, AWS Glue, and Redshift to automate ingestion and transformation of 500K+ daily sales and inventory records, enabling timely insights for business operations.",
            "Built scalable data infrastructure by structuring Snowflake schema models and organizing S3 zones, enhancing query performance by 12% and enabling faster access to analytics-ready datasets.",
            "Enhanced dimensional data models in Redshift to support scalable analytics, applying star schema design patterns and optimizing table structures for complex ad-hoc queries."
        ]
    },
    {
        id: 2,
        title: "Data Engineer",
        company: "NTTData",
        yearsActive: "Aug 2020 – Jul 2022",
        information: [
            "Created Azure Data Factory pipelines to ingest data from on-prem Oracle, PostgreSQL, and flat files into Azure Synapse Analytics and SQL Database, reducing ingestion time by 5%.",
            "Constructed scalable ETL processes to manage 200K+ daily financial records across Azure Data Lake, Synapse, and SQL Database, streamlining batch processing and ensuring end-to-end data integrity across analytical layers.",
            "Integrated Apache Kafka pipelines to support real-time and micro-batch ingestion of transactional data into Azure Data Lake, enhancing fraud detection capabilities and reducing latency in downstream analysis."
        ]
    },
    {
        id: 3,
        title: "Data Engineer",
        company: "NTTData",
        yearsActive: "Nov 2019 – Jul 2020",
        information: [
            "Contributed to large-scale data migration of 500K+ records from Oracle and SQL Server to Teradata, applying partitioning and indexing strategies to improve risk analytics query performance by 8%.",
            "Managed network element manager tasks utilizing Hadoop, HDFS, Hive, HBase, Zookeeper, Kafka, Apache Spark, and MapReduce, using Shell Scripting and Python Libraries.",
            "Demonstrated anomaly detection logics using PySpark and Pandas to identify 5–10% of inconsistent records early in the pipeline, boosting data reliability and reducing manual cleansing."
        ]
    },
    {
        id: 4,
        title: "M.S. in Computer Science",
        company: "Campbellsville University, KY",
        yearsActive: "Aug 2022 – May 2024",
        type: "education",
        information: [
            "Focused on data engineering, big data, and cloud platforms.",
            "Completed projects using Spark, cloud services, Python, and Power BI."
        ]
    }
];

export default WorkExperience;
